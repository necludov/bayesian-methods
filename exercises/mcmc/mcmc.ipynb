{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import trange\n",
    "from functools import partial\n",
    "from jax.scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture of Gaussians (MOG)\n",
    "\n",
    "The first target density to design our MCMC algorithms is the mixture of Gaussians\n",
    "$$p(x) = \\sum_i w_i\\mathcal{N}(x|\\mu_i,\\sigma_i^2)\\,.$$\n",
    "In practice, we always implement $\\log p(x)$ instead of the density because the gradient of density vanishes very quickly unlike gradients of the log-density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob_mog(x, means, covs, weights):\n",
    "  diff = x[:, jnp.newaxis, :] - means[jnp.newaxis, ...]  # Shape: (batch_size, n_modes, dim)\n",
    "  sol = jnp.linalg.solve(covs[jnp.newaxis, ...], diff[..., jnp.newaxis])  # Shape: (batch_size, n_modes, dim, 1)\n",
    "  log_prob = -0.5 * jnp.squeeze(jnp.matmul(diff[..., jnp.newaxis, :], sol), axis=-1).squeeze(-1)  # Shape: (batch_size, n_modes)\n",
    "  _, logdet_cov = jnp.linalg.slogdet(covs)\n",
    "  log_prob -= 0.5 * (x.shape[-1] * jnp.log(2. * jnp.pi) + logdet_cov)[jnp.newaxis, :]\n",
    "  log_prob += jnp.log(weights / weights.sum())[None, :]\n",
    "  return logsumexp(log_prob, axis=1)\n",
    "\n",
    "radius = 3.0\n",
    "scale = 0.1\n",
    "means = jnp.stack([\n",
    "      radius * jnp.array([\n",
    "          jnp.sin(i * jnp.pi / 4),\n",
    "          jnp.cos(i * jnp.pi / 4)\n",
    "      ]) for i in range(8)\n",
    "  ])\n",
    "covs = scale * jnp.stack([jnp.eye(2)] * 8)\n",
    "weights=(1/8)*jnp.ones(8,)\n",
    "logp = lambda x: log_prob_mog(x, means, covs, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_density():\n",
    "  x,y = jnp.linspace(-5,5,75), jnp.linspace(-5,5,75)\n",
    "  x,y = jnp.meshgrid(x,y)\n",
    "  grid = jnp.stack([x, y]).transpose((1,2,0)).reshape((-1,2))\n",
    "  plt.xlim(-5,5)\n",
    "  plt.ylim(-5,5)\n",
    "  plt.contourf(x,y,jnp.exp(logp(grid)).reshape(x.shape), levels=50)\n",
    "  \n",
    "plt.figure(figsize=(6, 5))\n",
    "key = jax.random.PRNGKey(1)\n",
    "key, init_key = jax.random.split(key)\n",
    "n_samples = 1000\n",
    "x_init = 1+0.1*jax.random.normal(init_key, shape=(n_samples, 2))\n",
    "  \n",
    "plot_density()\n",
    "plt.scatter(x_init[:,0], x_init[:,1], color=\"lightblue\", alpha=0.7, label='initial samples')\n",
    "plt.title(\"Target Distribution p(x)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Walk Metropolis Hastings (RWMH) (5 points)\n",
    "\n",
    "Implement the Metropolis-Hastings algorithm with Random Walk proposal. Recall that the Metropolis-Hastings algorithm is\n",
    "1. generate proposal $x' \\sim q(x'|x)$,\n",
    "2. evaluate the acceptance probability $$P_{\\text{accept}} = \\min\\bigg\\{1,\\frac{p(x')q(x|x')}{p(x)q(x'|x)}\\bigg\\}$$,\n",
    "3. set next sample to $x'$ with probability $P_{\\text{accept}}$ or to $x$ with probability $1-P_{\\text{accept}}$.\n",
    "\n",
    "Random Walk proposal in our case is simply\n",
    "$$q(x'|x) = \\mathcal{N}(x'|x,\\tau)\\,,$$\n",
    "where $\\tau$ is the step-size hyperparameter.\n",
    "\n",
    "The function `RWMH` implements one step of Random Walk Metropolis Hastings for many parallel chains. It outputs next samples and the Acceptance Rate, which is the average number of the accepted samples across all the chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=1)\n",
    "def RWMH(key, logp, x, tau):\n",
    "    # YOUR CODE HERE\n",
    "    return x_next, AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 0.1\n",
    "n_iter = 1000\n",
    "save_every = n_iter//5\n",
    "\n",
    "x = x_init.copy()\n",
    "xs, ar_plot = [], []\n",
    "for i in trange(n_iter):\n",
    "    key, loc_key = jax.random.split(key)\n",
    "    x, AR = RWMH(loc_key, logp, x, tau)\n",
    "    ar_plot.append(AR)\n",
    "    \n",
    "    if (i % save_every == 0) and i != 0:\n",
    "        xs.append(x)\n",
    "\n",
    "xs.append(x)\n",
    "xs = jnp.stack(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(28, 5))\n",
    "for i in range(int(n_iter/save_every)):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plot_density()\n",
    "    plt.scatter(xs[i, :, 0], xs[i, :, 1], color=\"lightblue\", alpha=0.7)\n",
    "    plt.title(f\"Iter: {(i + 1)* save_every}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(121)\n",
    "plot_density()\n",
    "plt.scatter(x[:,0], x[:,1], color=\"lightblue\", alpha=0.7)\n",
    "plt.subplot(122)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Acceptance Rate')\n",
    "plt.plot(ar_plot)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse the plots \n",
    "- How does the distribution of samples look like? Does it match the target density?\n",
    "- What happens with the Acceptance Rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Step Size (2 points)\n",
    "\n",
    "Based on the estimated Acceptance Rate, at every iteration, adjust the step size to keep the acceptance rate in the interval $[0.5,0.55]$. There are multiple ways you can adjust the step size and there is no right one. Try different ways, look at the plots, converge on the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 0.1\n",
    "n_iter = 1000\n",
    "save_every = n_iter//5\n",
    "\n",
    "x = x_init.copy()\n",
    "xs, ar_plot, tau_plot = [], [], []\n",
    "for i in trange(n_iter):\n",
    "    key, loc_key = jax.random.split(key)\n",
    "    x, AR = RWMH(loc_key, logp, x, tau)\n",
    "    # YOUR CODE HERE\n",
    "    ar_plot.append(AR)\n",
    "    tau_plot.append(tau)\n",
    "    \n",
    "    if (i % save_every == 0) and i != 0:\n",
    "        xs.append(x)\n",
    "\n",
    "xs.append(x)\n",
    "xs = jnp.stack(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(28, 5))\n",
    "for i in range(int(n_iter/save_every)):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plot_density()\n",
    "    plt.scatter(xs[i, :, 0], xs[i, :, 1], color=\"lightblue\", alpha=0.7)\n",
    "    plt.title(f\"Iter: {(i + 1)* save_every}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "plt.subplot(131)\n",
    "plot_density()\n",
    "plt.scatter(x[:,0], x[:,1], color=\"lightblue\", alpha=0.7)\n",
    "plt.subplot(132)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Acceptance Rate')\n",
    "plt.plot(ar_plot)\n",
    "plt.grid()\n",
    "plt.subplot(133)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Step Size')\n",
    "plt.plot(tau_plot)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- Argue for your choice of the adaptive step size technique. Does it keep the acceptance rate within the target interval?\n",
    "- Does keeping the acceptance rate within the target interval helps sampling? What's the intuition for this? Isn't it better to always accept samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unadjusted-Langevin Algorithm (ULA) (3 points)\n",
    "\n",
    "Implement the Langevin Dynamics, i.e. Euler discretization of the following SDE\n",
    "$$dx = \\nabla_x \\log p(x) dt + \\sqrt{2}dW_t\\,,$$ \n",
    "where $dt$ is the step size, and $dW_t$ is the standard Wiener process. The time-discretized scheme is\n",
    "$$x_{t+\\tau} = x_t + \\nabla_x \\log p(x_t) \\tau + \\sqrt{2\\tau}\\varepsilon \\;\\;\\varepsilon \\sim \\mathcal{N}(0,1)\\,.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=1)\n",
    "def ULA(key, logp, x, tau):\n",
    "    # YOUR CODE HERE\n",
    "    return x_next, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 0.1\n",
    "n_iter = 1000\n",
    "save_every = n_iter//5\n",
    "\n",
    "x = x_init.copy()\n",
    "xs, ar_plot = [], []\n",
    "for i in trange(n_iter):\n",
    "    key, loc_key = jax.random.split(key)\n",
    "    x, AR = ULA(loc_key, logp, x, tau)\n",
    "    ar_plot.append(AR)\n",
    "    \n",
    "    if (i % save_every == 0) and i != 0:\n",
    "        xs.append(x)\n",
    "\n",
    "xs.append(x)\n",
    "xs = jnp.stack(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(28, 5))\n",
    "for i in range(int(n_iter/save_every)):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plot_density()\n",
    "    plt.scatter(xs[i, :, 0], xs[i, :, 1], color=\"lightblue\", alpha=0.7)\n",
    "    plt.title(f\"Iter: {(i + 1)* save_every}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- Does ULA sample from the target distribution, i.e. does the distribution of samples match the target density?\n",
    "- How does it compare with RWMH? What are the main differences with RWMH?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metropolis-Adjusted Langevin Algorithm (MALA) (5 points)\n",
    "\n",
    "Implement the Metropolish Hastings algorithm where for the proposal we use the Langevin Dynamics, i.e.\n",
    "$$q(x'|x) = \\mathcal{N}(x'|x + \\nabla_x \\log p(x) \\tau, 2\\tau)\\,.$$ \n",
    "Use the adaptive step size to keep the acceptance rate in the interval $[0.5,0.55]$ as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jax.jit, static_argnums=1)\n",
    "def MALA(key, logp, x, tau):\n",
    "    # YOUR CODE HERE\n",
    "    return x_next, AR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 0.1\n",
    "n_iter = 1000\n",
    "save_every = n_iter//5\n",
    "\n",
    "x = x_init.copy()\n",
    "xs, ar_plot, tau_plot = [], [], []\n",
    "for i in trange(n_iter):\n",
    "    key, loc_key = jax.random.split(key)\n",
    "    x, AR = MALA(loc_key, logp, x, tau)\n",
    "    # YOUR CODE HERE\n",
    "    ar_plot.append(AR)\n",
    "    tau_plot.append(tau)\n",
    "    \n",
    "    if (i % save_every == 0) and i != 0:\n",
    "        xs.append(x)\n",
    "\n",
    "xs.append(x)\n",
    "xs = jnp.stack(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(28, 5))\n",
    "for i in range(int(n_iter/save_every)):\n",
    "    plt.subplot(1, 5, i + 1)\n",
    "    plot_density()\n",
    "    plt.scatter(xs[i, :, 0], xs[i, :, 1], color=\"lightblue\", alpha=0.7)\n",
    "    plt.title(f\"Iter: {(i + 1)* save_every}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "plt.subplot(131)\n",
    "plot_density()\n",
    "plt.scatter(x[:,0], x[:,1], color=\"lightblue\", alpha=0.7)\n",
    "plt.subplot(132)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Acceptance Rate')\n",
    "plt.plot(ar_plot)\n",
    "plt.grid()\n",
    "plt.subplot(133)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Step Size')\n",
    "plt.plot(tau_plot)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- How does it perform compared to ULA and RWMH?\n",
    "- Does your step size adjustment keep the acceptance rate within the target interval?\n",
    "- Which algorithm out of the three implemented would you use for this task? Maybe a combination of the algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Logistic Regression (7 points)\n",
    "\n",
    "Finally, use the implemented MCMC algorithms to sample from the posterior distribution of the Bayesian Logistic Regression. Namely, consider the following probabilistic model. The likelihood is\n",
    "$$p(y| x,\\theta) = \\sigma(w^Tx + b)^{y=1}(1-\\sigma(w^Tx + b))^{y=0}\\,,$$\n",
    "where the vector of parameters $\\theta = (w,b)$. The prior is\n",
    "$$p(\\theta) = p(w) = \\mathcal{N}(w|0,1)\\,.$$\n",
    "Implement the posterior distribution log-density\n",
    "$$\\log p(\\theta|D) = \\sum_i \\log p(y_i|x_i,\\theta) + \\log p(\\theta)\\,.$$\n",
    "Implement the evaluation of accuracy for the predictive distribution\n",
    "$$p(y|x) = \\mathbb{E}_{p(\\theta|D)}p(y|x,\\theta) \\approx \\frac{1}{n}\\sum_{i=1}^n p(y|x,\\theta_i)\\,, \\theta_i \\sim p(\\theta|D)\\,.$$\n",
    "\n",
    "Beware of numerical instabilities cause by zeros in the logarithms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data.npy').squeeze()\n",
    "labels = np.load('labels.npy').squeeze()\n",
    "dm = np.mean(data, axis=0)\n",
    "ds = np.std(data, axis=0)\n",
    "data = (data - dm) / ds\n",
    "num_features = data.shape[1]  \n",
    "\n",
    "def logp(theta):\n",
    "  # YOUR CODE HERE\n",
    "\n",
    "def get_accuracy(theta):\n",
    "  # YOUR CODE HERE\n",
    "\n",
    "key, init_key = jax.random.split(key)\n",
    "theta_init = jax.random.normal(init_key, (n_samples,num_features+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these ground true values were estimated using a very long markov chain\n",
    "gt_mean = np.array([-0.13996868,  0.71390106,  0.69571619,  0.43944853,  0.36997702, -0.27319424,\n",
    "                    0.31730518, -0.49617367,  0.40516419, 0.4312388,   0.26531786, 1.10337417,\n",
    "                    0.70054367, -0.25684964])\n",
    "gt_std = np.array([0.22915648,  0.24545612,  0.20457998,  0.20270157,  0.21040644,  0.20094482,\n",
    "                   0.19749419,  0.24134014,  0.20230987,  0.25595334,  0.23709087,  0.24735325,\n",
    "                   0.20701178,  0.19771984])\n",
    "\n",
    "def run_MCMC(key, algorithm, adaptive_AR=True):\n",
    "  tau = 0.02\n",
    "  n_iter = 1000\n",
    "  save_every = 5\n",
    "\n",
    "  theta = theta_init.copy()\n",
    "  thetas, ar_plot, tau_plot = [], [], []\n",
    "  for i in trange(n_iter):\n",
    "      key, loc_key = jax.random.split(key)\n",
    "      theta, AR = algorithm(loc_key, logp, theta, tau)\n",
    "      if adaptive_AR:\n",
    "        # YOUR CODE HERE\n",
    "      ar_plot.append(AR)\n",
    "      tau_plot.append(tau)\n",
    "      \n",
    "      if (i % save_every == 0) and i != 0:\n",
    "          thetas.append(theta)\n",
    "\n",
    "  thetas.append(theta)\n",
    "  thetas = jnp.stack(thetas)\n",
    "  return thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collecting samples using different algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key, *loc_keys = jax.random.split(key, 4)\n",
    "thetas_rwmh = run_MCMC(loc_keys[0], RWMH)\n",
    "thetas_ula = run_MCMC(loc_keys[1], ULA, False)\n",
    "thetas_mala = run_MCMC(loc_keys[2], MALA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def get_metrics(samples):\n",
    "  err_mean = ((samples.mean(0)-gt_mean)**2).sum()\n",
    "  err_std = ((samples.std(0)-gt_std)**2).sum()\n",
    "  acc = get_accuracy(samples)\n",
    "  return err_mean, err_std, acc\n",
    "\n",
    "def evaluate_samples(thetas):\n",
    "  err_plot_mean, err_plot_std, acc_plot = [], [], []\n",
    "  for i in range(1,len(thetas)+1):\n",
    "    err_mean, err_std, acc = get_metrics(thetas[:i].reshape(-1,thetas.shape[-1]))\n",
    "    err_plot_mean.append(err_mean)\n",
    "    err_plot_std.append(err_std)\n",
    "    acc_plot.append(acc)\n",
    "  return {'mean': err_plot_mean, 'std': err_plot_std, 'acc': acc_plot}\n",
    "\n",
    "plots_rwmh = evaluate_samples(thetas_rwmh)\n",
    "plots_ula = evaluate_samples(thetas_ula)\n",
    "plots_mala = evaluate_samples(thetas_mala)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "plt.subplot(131)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Error of the Estimated Mean')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.plot(10**3*np.arange(1,len(thetas_rwmh)+1), plots_rwmh['mean'], label='RWMH')\n",
    "plt.plot(10**3*np.arange(1,len(thetas_ula)+1), plots_ula['mean'], label='ULA')\n",
    "plt.plot(10**3*np.arange(1,len(thetas_mala)+1), plots_mala['mean'], label='MALA')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.subplot(132)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Error of the Estimated Standard Deviation')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.plot(10**3*np.arange(1,len(thetas_rwmh)+1), plots_rwmh['std'], label='RWMH')\n",
    "plt.plot(10**3*np.arange(1,len(thetas_ula)+1), plots_ula['std'], label='ULA')\n",
    "plt.plot(10**3*np.arange(1,len(thetas_mala)+1), plots_mala['std'], label='MALA')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.subplot(133)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('Classification Accuracy')\n",
    "plt.xscale('log')\n",
    "plt.plot(10**3*np.arange(1,len(thetas_rwmh)+1), plots_rwmh['acc'], label='RWMH')\n",
    "plt.plot(10**3*np.arange(1,len(thetas_ula)+1), plots_ula['acc'], label='ULA')\n",
    "plt.plot(10**3*np.arange(1,len(thetas_mala)+1), plots_mala['acc'], label='MALA')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- Which algorithm performs the best for the estimation of the ground true mean and std of the posterior distribution? What's the intuition for this?\n",
    "- Which algorithm performs the best for predicting the labels (in terms of the accuracy)? What's the intuition for this?\n",
    "- Did your ranking of the algorithms changed compared to the Mixture of Gaussians? Why?\n",
    "- What are you going to do if you get a \"black box\" density?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('jax-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "9b5010b7c43642655f9a773276a2f783938f8332af16a6f04f16ff491d6a6741"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
